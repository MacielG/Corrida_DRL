# âš¡ QuickStart - ComeÃ§ar em 5 Minutos

## 1ï¸âƒ£ InstalaÃ§Ã£o (1 min)

```bash
# Clone
git clone https://github.com/MacielG/Corrida_DRL.git
cd Corrida_DRL

# Ambiente virtual
python -m venv venv
source venv/bin/activate  # ou venv\Scripts\activate no Windows

# DependÃªncias
pip install -r requirements.txt
```

## 2ï¸âƒ£ Execute o Jogo (3 min)

```bash
python main.py
```

**O que acontece:**
1. Menu principal abre
2. Clique "Assistir Corrida" ou "Treinar"
3. Selecione agente (ou crie um em "GestÃ£o de Agentes")
4. Escolha mapa (Corredor, Curva, Circular)
5. Veja a corrida com dashboard em tempo real

## 3ï¸âƒ£ Com ConfiguraÃ§Ã£o Customizada (1 min)

```bash
# Use arquivo de config
cp config_example.yaml meu_config.yaml

# Edite meu_config.yaml com seus parÃ¢metros

# Execute
python main.py --config meu_config.yaml
```

---

## ğŸ“Š Monitorar Treinamento

### TensorBoard
```bash
# Terminal 1: Treino
python main.py

# Terminal 2: Visualizar (abra http://localhost:6006)
tensorboard --logdir tensorboard_logs
```

### MLflow
```bash
# Terminal 1
mlflow ui  # Abra http://localhost:5000

# Terminal 2
python main.py
```

---

## ğŸ§ª Rodar Testes

```bash
# Todos
pytest tests/ -v

# Apenas core
pytest tests/test_core_module.py -v

# Com cobertura
pytest --cov=core
```

---

## ğŸ“ Estrutura BÃ¡sica

```
Corrida_DRL/
â”œâ”€â”€ main.py                 # Ponto de entrada
â”œâ”€â”€ agent.py               # Agente RL
â”œâ”€â”€ environment.py         # Simulador
â”œâ”€â”€ core/                  # MÃ³dulos avanÃ§ados
â”‚   â”œâ”€â”€ config_manager.py  # ConfiguraÃ§Ã£o
â”‚   â”œâ”€â”€ reward_shaper.py   # Recompensas
â”‚   â””â”€â”€ callbacks.py       # Monitoramento
â”œâ”€â”€ tests/                 # Testes
â”œâ”€â”€ models/                # Modelos salvos
â”œâ”€â”€ tensorboard_logs/      # TensorBoard
â””â”€â”€ config_example.yaml    # Exemplo de config
```

---

## ğŸ’¡ PrÃ³ximos Passos

### Trocar Algoritmo
Edite `config_example.yaml`:
```yaml
algorithm:
  name: "PPO"  # ou "SAC"
```

### Trocar FunÃ§Ã£o de Recompensa
```python
from core.reward_shaper import RewardShapeFactory

# OpÃ§Ãµes: 'balanced', 'speed', 'safety'
shaper = RewardShapeFactory.create('speed')
```

### Usar Melhor Modelo Treinado
```bash
python main.py --skip-training
```

---

## â“ Troubleshooting

**"ModuleNotFoundError: No module named 'core'"**
â†’ Certifique-se que vocÃª estÃ¡ no diretÃ³rio `Corrida_DRL`

**"Port 6006 already in use"**
â†’ TensorBoard rodando em outra janela. Feche ou use porta diferente:
```bash
tensorboard --logdir tensorboard_logs --port 6007
```

**Jogo muito lento**
â†’ Reduza `n_parallel` em config.yaml (padrÃ£o Ã© 4)

---

## ğŸ¯ Exemplo Completo

```bash
# 1. Clone
git clone https://github.com/MacielG/Corrida_DRL.git && cd Corrida_DRL

# 2. Setup
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt

# 3. Configure
cp config_example.yaml config.yaml
# Edite config.yaml se quiser customizar

# 4. Treino
python main.py --config config.yaml

# 5. Monitore (em outro terminal)
tensorboard --logdir tensorboard_logs

# 6. Abra navegador
# http://localhost:6006
```

**Pronto! VocÃª tem um agente RL treinando em uma pista de corrida com monitoramento em tempo real! ğŸï¸**

---

## ğŸ“š Saiba Mais

- **README_PRODUCTION.md** - DocumentaÃ§Ã£o completa
- **CORRECOES_FLUXO_E_VISUAL.md** - Ãšltimas correÃ§Ãµes
- **config_example.yaml** - Todos os parÃ¢metros disponÃ­veis

---

**DÃºvidas? Abra uma Issue no GitHub! ğŸ›**
