# ü§ù Contribuindo para Corrida DRL

Obrigado por considerar contribuir! Este documento descreve como fazer isso.

## üìã √çndice
1. [C√≥digo de Conduta](#c√≥digo-de-conduta)
2. [Como Contribuir](#como-contribuir)
3. [Desenvolvimento Local](#desenvolvimento-local)
4. [Padr√µes de C√≥digo](#padr√µes-de-c√≥digo)
5. [Submeter PR](#submeter-pr)
6. [Reportar Bugs](#reportar-bugs)

---

## üìú C√≥digo de Conduta

Todos os colaboradores devem:
- ‚úÖ Ser respeitosos
- ‚úÖ Incluir diversidade
- ‚úÖ Focar em argumentos, n√£o em pessoas
- ‚úÖ Reportar comportamento inaceit√°vel para os mantenedores

Comportamento abusivo **n√£o √© tolerado** e resultar√° em exclus√£o.

---

## üí¨ Como Contribuir

### Reportar Bugs üêõ

1. **Procure** se algu√©m j√° reportou no Issues
2. **Descreva claramente**:
   - Vers√£o do Python e SO
   - Passos para reproduzir
   - Comportamento esperado vs observado
   - Screenshots/logs se relevante

**Exemplo:**
```
T√≠tulo: PPO falha em treino com n_parallel=8

Vers√£o: Python 3.11, Ubuntu 20.04
Passos:
1. python main.py --config config.yaml (com n_parallel: 8)
2. Selecione PPO
3. Inicie treino

Erro:
RuntimeError: CUDA out of memory

Esperado: Treino com 8 ambientes paralelos
```

### Sugerir Melhorias üí°

1. **Issue com r√≥tulo `enhancement`**
2. **Descreva claramente**:
   - Problema que resolve
   - Solu√ß√£o proposta
   - Exemplos de como seria usado
   - Alternativas consideradas

**Exemplo:**
```
T√≠tulo: Adicionar suporte a GPU com CUDA

Descri√ß√£o:
Atual: Treino √© apenas em CPU
Proposto: Detectar e usar GPU automaticamente

Impacto: 3-5x speedup em m√°quinas com GPU
```

### Submeter C√≥digo üöÄ

1. **Forque o reposit√≥rio**
2. **Crie branch**: `git checkout -b feature/sua-feature`
3. **Implemente** com testes
4. **Push** e abra PR

---

## üõ†Ô∏è Desenvolvimento Local

### Setup

```bash
# Clone seu fork
git clone https://github.com/SEU_USUARIO/Corrida_DRL.git
cd Corrida_DRL

# Crie virtual env
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Instale em modo desenvolvimento
pip install -e .
pip install -r requirements.txt
pip install pytest pytest-cov flake8 black
```

### Workflow

```bash
# Crie sua branch
git checkout -b feature/minha-feature

# Fa√ßa mudan√ßas
# ... edite arquivos ...

# Teste tudo
pytest tests/ -v
flake8 core tests
black core tests

# Commit
git add .
git commit -m "Adiciona minha feature"

# Push
git push origin feature/minha-feature
```

---

## üé® Padr√µes de C√≥digo

### Python Style

Seguimos PEP 8 com algumas flexibilidades:

```python
# ‚úÖ Bom
def compute_reward(position: Tuple[float, float], 
                   velocity: float) -> float:
    """Computa recompensa baseada em posi√ß√£o e velocidade."""
    reward = velocity * 0.5
    return reward

# ‚ùå Ruim
def computeReward(pos,vel):
    return vel*0.5
```

### Type Hints

```python
# ‚úÖ Com type hints
from typing import Dict, Optional, Tuple

def process_data(data: Dict[str, Any], 
                 threshold: Optional[float] = None) -> Tuple[bool, Dict]:
    pass

# ‚ùå Sem type hints
def process_data(data, threshold=None):
    pass
```

### Docstrings

```python
# ‚úÖ Bom
def train_agent(self, total_timesteps: int = 100000) -> Dict[str, float]:
    """Treina o agente com valida√ß√£o peri√≥dica.
    
    Args:
        total_timesteps: Total de passos para treinar.
        
    Returns:
        Dicion√°rio com estat√≠sticas de treinamento:
        - 'final_reward': Recompensa final m√©dia
        - 'training_time': Tempo total em segundos
        
    Raises:
        ValueError: Se total_timesteps < 1000.
    """
    pass

# ‚ùå Ruim
def train_agent(self, total_timesteps):
    # treina
    pass
```

### Testes

```python
# ‚úÖ Teste bem estruturado
class TestRewardShaper:
    """Testes para RewardShaper."""
    
    def test_checkpoint_reward(self):
        """Testa se recompensa de checkpoint √© aplicada."""
        shaper = BalancedRewardShaper()
        reward = shaper.compute_reward(
            position=(0, 0),
            velocity=5.0,
            angle=0.0,
            checkpoint_idx=1,
            total_checkpoints=5,
            collision=False,
            out_of_bounds=False,
            progress=0.0
        )
        assert reward > 0

# ‚ùå Teste fraco
def test_reward():
    r = compute_reward()
    assert r > 0
```

---

## üìù Submeter PR

### T√≠tulo
Ser descritivo e conciso:
- ‚úÖ "Adiciona BalancedRewardShaper com checkpoint detection"
- ‚ùå "Fix"

### Descri√ß√£o

```markdown
## Descri√ß√£o
Breve descri√ß√£o do que faz.

## Tipo de Mudan√ßa
- [ ] Bug fix (n√£o quebra funcionalidade)
- [ ] Nova feature (quebra compatibilidade?)
- [ ] Breaking change (explique por qu√™)
- [ ] Documenta√ß√£o

## Checklist
- [ ] Testes adicionados/atualizados
- [ ] Documenta√ß√£o atualizada
- [ ] C√≥digo passa em `flake8`
- [ ] Testes passam em `pytest`
- [ ] Branch est√° atualizado com `main`

## Teste Manual
Passos para testar:
1. ...
2. ...

## Screenshots (se aplic√°vel)
[Insira imagens]
```

### CI/CD

Seu PR ser√° validado por:
1. ‚úÖ **Pytest** - Testes unit√°rios
2. ‚úÖ **Flake8** - Estilo de c√≥digo
3. ‚úÖ **Coverage** - Cobertura de testes
4. ‚úÖ **Integration** - Testes de integra√ß√£o

Todos devem passar para merge.

---

## üéØ √Åreas Prontas para Contribui√ß√£o

### F√°cil (Bom para Iniciantes)
- [ ] Adicionar docstrings faltantes
- [ ] Melhorar mensagens de erro
- [ ] Adicionar exemplos no README
- [ ] Traduzir documenta√ß√£o

### M√©dio
- [ ] Novo RewardShaper customizado
- [ ] Novo tipo de mapa
- [ ] Melhora de performance
- [ ] Mais testes

### Avan√ßado
- [ ] Novo algoritmo RL (A3C, DDPG, etc)
- [ ] Transfer learning
- [ ] Multi-agent competi√ß√£o
- [ ] Integra√ß√£o com novo simulador

---

## üìö Estrutura para Adicionar Feature

### 1. Novo RewardShaper

```python
# core/reward_shaper.py
class MyRewardShaper(BaseRewardShaper):
    def compute_reward(self, **kwargs) -> float:
        # Sua implementa√ß√£o
        pass
    
    def reset(self) -> None:
        pass

# Registre
RewardShapeFactory.register('my_shaper', MyRewardShaper)
```

### 2. Novo Algoritmo

```python
# agent.py
class MyAgent(BaseAgent):
    def __init__(self, env, ...):
        super().__init__(env, ...)
        self.model = MyAlgorithm(...)  # SB3 ou customizado
    
    def train(self, total_timesteps: int) -> Dict:
        # Implementa√ß√£o
        pass
    
    def predict(self, observation):
        # Implementa√ß√£o
        pass
```

### 3. Novo Mapa

```python
# environment.py
def _setup_map(self):
    if self.map_type == "meu_mapa":
        self.corridor_rect = (...)
        self.barriers = [...]
        self.checkpoints = [...]
```

---

## üîç Code Review Process

1. **Dois reviewers** m√≠nimo (incluindo mantenedor)
2. **Feedback construtivo** - focar em c√≥digo, n√£o em pessoa
3. **Itera√ß√£o** - author responde coment√°rios
4. **Aprova√ß√£o** - ap√≥s todas mudan√ßas aprovadas
5. **Squash merge** - mant√©m hist√≥rico limpo

---

## üì¶ Release Process

Versioning: [Semantic Versioning](https://semver.org/)
- MAJOR.MINOR.PATCH (ex: 2.0.1)

Tags:
```bash
git tag v2.0.1
git push origin v2.0.1
```

---

## üìû Suporte e Contato

- **Issues**: https://github.com/MacielG/Corrida_DRL/issues
- **Discussions**: https://github.com/MacielG/Corrida_DRL/discussions
- **Email**: seu-email@example.com

---

## ‚ú® Reconhecimento

Contribuidores ser√£o listados em:
- README.md (se√ß√£o "Contribuidores")
- GitHub Contributors Page
- Release Notes

---

**Obrigado por contribuir! üéâ**

*√öltima atualiza√ß√£o: Novembro 2024*
