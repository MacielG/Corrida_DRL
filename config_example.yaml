# Exemplo de configuração YAML para Corrida DRL

algorithm:
  name: "PPO"
  learning_rate: 0.0003
  gamma: 0.98
  batch_size: 64
  policy: "MlpPolicy"

environment:
  map_type: "corridor"
  width: 600
  height: 400
  max_steps: 500
  render: false
  render_interval: 1

reward:
  checkpoint_reward: 100.0
  collision_penalty: -50.0
  speed_reward_factor: 0.5
  progress_reward_factor: 1.0
  stability_reward: 1.0
  out_of_bounds_penalty: -100.0

training:
  total_timesteps: 100000
  eval_interval: 5000
  n_parallel: 4
  n_eval_episodes: 10
  checkpoint_interval: 5000
  save_best_only: true
  verbose: 1

logging:
  log_dir: "logs"
  models_dir: "models"
  tensorboard_log: "tensorboard_logs"
  mlflow_tracking_uri: null
  mlflow_experiment_name: "corrida_drl"
  save_logs: true
  log_level: "INFO"
