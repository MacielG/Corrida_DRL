name: Tests & CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest]
        python-version: ['3.10', '3.11']
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install flake8 coverage
    
    - name: Lint with flake8
      run: |
        # Checa sintaxe
        flake8 core tests --count --select=E9,F63,F7,F82 --show-source --statistics
        # Aviso de estilo
        flake8 core tests --count --exit-zero --max-complexity=10 --max-line-length=100 --statistics
    
    - name: Run unit tests
      run: |
        pytest tests/ -v --tb=short --timeout=60
    
    - name: Generate coverage report
      run: |
        pytest tests/ --cov=core --cov-report=xml --cov-report=html --timeout=60
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  integration:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Test config loading
      run: |
        python -c "from core.config_manager import ConfigManager; c = ConfigManager('config_example.yaml'); print('✓ Config loaded')"
    
    - name: Test reward shapers
      run: |
        python -c "from core.reward_shaper import RewardShapeFactory; s = RewardShapeFactory.create('balanced'); print('✓ Reward shaper created')"
    
    - name: Quick integration test
      run: |
        pytest tests/test_core_module.py -v --timeout=30
    
    timeout-minutes: 10

  build-docker:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
    
    - name: Build Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        push: false
        tags: corrida-drl:latest

  security:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install bandit safety
    
    - name: Run Bandit security check
      run: |
        bandit -r core tests -ll -f csv > bandit-report.csv || true
    
    - name: Check dependencies
      run: |
        safety check --json || true

  metrics:
    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install coverage pytest
    
    - name: Run tests and collect metrics
      run: |
        pytest tests/ --cov=core --cov-report=term-missing --timeout=60
    
    - name: Count lines of code
      run: |
        find core -name "*.py" -type f | xargs wc -l | tail -1

  status:
    runs-on: ubuntu-latest
    needs: [test, integration]
    if: always()
    
    steps:
    - name: Check test results
      run: |
        if [ "${{ needs.test.result }}" == "failure" ] || [ "${{ needs.integration.result }}" == "failure" ]; then
          echo "❌ Testes falharam"
          exit 1
        else
          echo "✅ Todos os testes passaram"
        fi
